# -*- coding: utf-8 -*-
"""UWARG Bootcamp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FuxoLC-MAKYrI1M7_o04_4sQilQ7pMaU

# University of Waterloo Arial Robotics Group Computer Vision Bootcamp

This is a starter file to get you going. You may also include other files if you feel it's necessary.

Make sure to follow the code convention described here:
https://github.com/UWARG/computer-vision-python/blob/main/README.md#naming-and-typing-conventions

Hints:
* The internet is your friend! Don't be afraid to search for tutorials/intros/etc.
* We suggest using a convolutional neural network.
* TensorFlow Keras has the CIFAR-10 dataset as a module, so you don't need to manually download and unpack it.

# Imports and Setup
"""

import tensorflow as tf
import numpy as np
import pandas as pd

# Plotting and Data Visualization Libraries
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns

# Complex Iterator Library 
import itertools

"""# Data Loading and Pre-Processing"""

# Load the Dataset
data = tf.keras.datasets.cifar10

# Split Data into x_train, y_train, x_test, and y_test as a set of two tuples for training and validation sets
# x_train, x_test -> training and testing sets of 32x32 pixel images
# y_train, y_test -> labels for the respective images 

(x_train, y_train), (x_test, y_test) = data.load_data()

# Check Format
print(f"x_train: {x_train.shape}\nx_test: {x_test.shape}")

"""
Based on the above output, we can see that our training sets are 50000 images each with 32x32 pixel images. 
Our test sets however are much smaller with around 10000 images of the same type.
"""

# Flatten Label Data to be passed to the input layers we want 1D data for fully connected layers in the Feedforward Neural Network structure
y_train = y_train.flatten()
y_test = y_test.flatten()

# Class Labels
classes = ['airplane', 
           'automobile',
           'bird',
           'cat',
           'deer',
           'dog',
           'frog',
           'horse',
           'ship',
           'truck']

# Plot the Data
plt.figure(figsize=(20,10))
plot = sns.countplot(y_train)
plot.set(title = "Class Image Counts")
plot.set(xticklabels=classes)
plot.set(xlabel="Classes")
plot.set(ylabel="Amount of Images")

"""From the above plot, we can observe that each class has a total of 5000 images.
Given that there are 10 classes, we have a total of 50,000 images.

# Data Normalization and Reshaping
"""

# Since we're constructing a CNN, we want normalized values for our data; values that range from 0 to 1
# Our data as it is right now, ranges from 0-255

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)
x_train = x_train/255.0
x_test = x_test/255.0

# Using one-hot encoding, more features can be added to the categorical variables which are the labels
y_train = tf.one_hot(y_train.astype(np.int32), depth=len(classes))
y_test = tf.one_hot(y_test.astype(np.int32), depth=len(classes))

# Here is an example image with it's respective label 
plt.imshow(x_train[0])
print(y_train[0])

"""The above image is a frog

# Building and Evaluating the Model
"""

# Overfitting on the data is a risk that is enabled by the below parameters
# Training for too long will develop biases within the model, and undertraining
# Will affect the accuracy simply because there isn't enough confidence to make
# a valid prediction 

# There are techniques to prevent overfitting the model such as data augmentation
# Where artificial data is generated by altering existing data 
# (No Data augmentation will be employed in this notebook)

# Defining Constants
batch_size = 32
class_count = len(classes)
epochs = 30

# Model Desgin:
# 2 Convolution Layers with 32 Filters
# Pooling Layer
# 2 Convolution Layers with 64 Filters
# Pooling Layer
# Flatten Layer to convert data to 1D for Dense Layer Processing 
# 2 Dense Layers one of which is an Output Layer

# Pooling layers take the max float value from a 2x2 matrix of values and produces
# One pixel 

input_shape = (32, 32, 3)

# Define the Model - Sequential Model Structure
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(filters = 32,
                           kernel_size = 3,
                           padding = "same",
                           input_shape = x_train.shape[1:],
                           activation = "relu"),
    tf.keras.layers.Conv2D(filters = 32,
                           kernel_size = 3,
                           activation = "relu"),
    tf.keras.layers.MaxPool2D(),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Conv2D(filters = 64,
                           kernel_size = 3,
                           padding = "same",
                           activation = "relu"),
    tf.keras.layers.Conv2D(filters = 64,
                           kernel_size = 3,
                           activation = "relu"),
    tf.keras.layers.MaxPool2D(),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Flatten(),

    tf.keras.layers.Dense(units = 512, activation = "relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units = class_count, activation = "softmax")
])

# Using the Adam optimizer for a fast optimization algorithm to achieve the desired output 

# Compile the Model
model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-03, decay = 1e-05),
              loss = "categorical_crossentropy", metrics = ["accuracy"])

# Fit the Training Data

training = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs)

# Plot Accuracy and Loss with respect to the current epoch
fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2)
fig.tight_layout()

ax1.plot(training.history["accuracy"])
ax2.plot(training.history["loss"])

ax1.set(title = "Accuracy Overtime")
ax2.set(title = "Loss Overtime")
ax1.set(xlabel = "epochs")
ax1.set(ylabel = "accuracy")
ax2.set(xlabel = "epochs")
ax2.set(ylabel = "loss")

# Evaluating the Model's Performance Overtime

test_accuracy, test_loss = model.evaluate(x_test, y_test)